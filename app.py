# app.py (Version avec indicateur de progression)

# --- DEBUT DU PATCH POUR SQLITE ---
# Correction pour forcer l'utilisation d'une version r√©cente de sqlite3
# compatible avec ChromaDB sur des environnements comme Streamlit Cloud.
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# --- FIN DU PATCH ---

import streamlit as st
import os
import time
from rag_agent import RAGAgent
from index import create_embeddings_store

# --- Configuration de la page ---
st.set_page_config(page_title="Assistant √âconomique", layout="wide")

st.title("ü§ñ Assistant d'Analyse √âconomique et Financi√®re")
st.markdown("Bas√© sur des rapports de l'OCDE, FMI, BCE, Fed, etc.")

# --- Logique de chargement et de construction de la base de donn√©es ---
DB_PATH = "./chroma_db"

@st.cache_resource
def initialize_agent():
    if not os.path.exists(DB_PATH):
        with st.spinner("Base de donn√©es non trouv√©e. Lancement de l'indexation des documents... (cette op√©ration peut prendre plusieurs minutes)"):
            try:
                create_embeddings_store()
            except Exception as e:
                st.error(f"Erreur lors de la cr√©ation de la base de donn√©es : {e}")
                return None
    
    with st.spinner("Initialisation de l'agent RAG..."):
        agent = RAGAgent()
    return agent

agent = initialize_agent()

if agent:
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Je suis pr√™t. Posez-moi une question sur l'√©conomie."}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Posez votre question ici..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # --- D√âBUT DE LA NOUVELLE LOGIQUE D'AFFICHAGE AVEC PROGRESSION ---
        with st.chat_message("assistant"):
            final_answer = ""
            final_sources = []
            
            # 1. On cr√©e un conteneur st.status pour afficher la progression
            with st.status("Lancement du processus...", expanded=True) as status:
                try:
                    # 2. On boucle sur le "stream" d'√©v√©nements de l'agent
                    for event in agent.stream_query(prompt):
                        # On v√©rifie quel n≈ìud vient de se terminer
                        if "on_chain_end" in event['event']:
                            node_name = event['name']
                            # 3. On met √† jour le message en fonction de l'√©tape
                            if node_name == "relevance_check":
                                status.update(label="‚úÖ Pertinence v√©rifi√©e. Classification de la question...", state="running")
                            elif node_name == "classify_question":
                                status.update(label="‚úÖ Classification termin√©e. Lancement de la recherche...", state="running")
                            elif node_name in ["search", "hybrid_search", "web_search"]:
                                status.update(label="‚úÖ Recherche termin√©e. Construction du contexte...", state="running")
                            elif node_name == "context":
                                status.update(label="‚úÖ Contexte assembl√©. G√©n√©ration de la r√©ponse...", state="running")
                            elif node_name == "answer":
                                status.update(label="‚úÖ R√©ponse g√©n√©r√©e. Examen par la critique...", state="running")
                            elif node_name == "critique_answer":
                                status.update(label="‚úÖ Critique termin√©e.", state="running")

                        # On r√©cup√®re le r√©sultat final lorsque le graphe se termine
                        if event['event'] == 'on_graph_end':
                            final_result = event['data']['output']
                            final_answer = final_result.get("answer", "Une erreur est survenue.")
                            final_sources = final_result.get('source_documents', [])
                            status.update(label="Processus termin√© !", state="complete", expanded=False)

                except Exception as e:
                    logger.error(f"Erreur lors du streaming de la requ√™te : {e}", exc_info=True)
                    final_answer = f"Une erreur est survenue pendant le traitement : {e}"

            # 4. On affiche la r√©ponse finale et les sources
            full_response = final_answer
            if final_sources:
                full_response += "\n\n---\n**üìö Sources utilis√©es :**\n"
                for i, doc in enumerate(final_sources):
                    source_name = doc.metadata.get('source', 'Source inconnue')
                    page = doc.metadata.get('page')
                    display_name = f"* `[{i+1}]` {source_name}"
                    if page is not None:
                        display_name += f" (Page: {int(page) + 1})"
                    full_response += display_name + "\n"
            
            st.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
        # --- FIN DE LA NOUVELLE LOGIQUE D'AFFICHAGE ---
else:
    st.error("L'initialisation de l'agent a √©chou√©. L'application ne peut pas continuer.")